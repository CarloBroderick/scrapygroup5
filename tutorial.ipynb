{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Web Scraping with Scrapy üåê\n",
    "\n",
    "<img src=\"./assets/scrapy.png\" alt=\"scrapy\" width=\"800\"/>\n",
    "\n",
    "[Scrapy](https://pandas.pydata.org/docs/index.html) is \"an open source and collaborative framework for extracting the data you need from websites\". It is written in Python and runs on Linux, Windows, Mac and BSD. Scrapy is one of the main tools for web scraping on Python, in addition to other tools like Beautiful Soup and Selenium (which automates and extracts data). It is preffered \n",
    "\n",
    "This exercise covers the basics of Scrapy, inlcuding how to load the library, read a `DataFrame`, extract a `DataFrame`, and save a `DataFrame` as a .csv file. While this notebook is designed to get you started on Scrapy, the [docs](https://docs.scrapy.org/en/latest/) should the first place you look for documentation and additional Scrapy functionality.\n",
    "\n",
    "**Credit:** This notebook follows the example presented [here](https://github.com/ifrankandrade/data-collection) by Frank Andrade.\n",
    "\n",
    "<hr style=\"border-top: 0.2px solid gray; margin-top: 12pt; margin-bottom: 0pt\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment on Anaconda\n",
    "[Anaconda](https://www.anaconda.com/) simplifies package management and deployment. Here, we will load the Scrapy library.\n",
    "<ol>\n",
    "<li>Navigate to the <b>Environments</b> section on the left panel</li>\n",
    "<li>Clone your <b>eds-217</b> environment and assign it a new name. For example, <b>eds-217-scrapy</b></li>\n",
    "<li>Open the terminal via your new environment. Click on the green arrow next to <b>eds-217-scrapy</b> on the left panel</li>\n",
    "</ol>\n",
    "\n",
    "On the terminal, run the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | ^C\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Scrapy on VS Code\n",
    "We work on <a href = \"https://code.visualstudio.com/\">Visual Studio (VS) Code</a> but you have other integrated development environment options like Pycharm. Here, we set up our workspace with a GitHub repository and specify our environment. \n",
    "<ol>\n",
    "<li>Create a new repository on GitHub called eds-217-scrapy</li>\n",
    "<li>Clone to make a version controlled workspace on VS Code</li>\n",
    "<li>Create a Jupyter notebook by <i>New text file</i> > <i>Save as</i> > <i>Assign a name and add file extentsion \".ipynb\"</i>\n",
    "<li>Navigate to the environment on the upper right corner of the Jupyter notebook and change Kernel to <b>eds-217-scrapy</b></li>\n",
    "<li>Next, if you are on macOS, update your terminal to your new environment. Run the line:</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda activate eds-217-scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get to know Scrapy\n",
    "Let's learn more about Scrapy commands. To view library doucmentation and available commands, on your terminal run the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command       Use                                                                               Type\n",
      "------------  --------------------------------------------------------------------------------  --------------\n",
      "startproject  creates a new Scrapy project                                                      global command\n",
      "genspider     creates a new spider on the current folder                                        global command\n",
      "settings      gets the value of a Scrapy setting                                                global command\n",
      "runspider     run a spider self-contained in a Python file, without having to create a project  global command\n",
      "shell         starts the Scrapy shell for the given URL                                         global command\n",
      "fetch         downloads the given URL and writes the contents to standard output                global command\n",
      "view          Opens the given URL in a browser, as your Scrapy spider would see it              global command\n",
      "version       prints the Scrapy version                                                         global command\n",
      "crawl         starts crawling with spider                                                       local command\n",
      "check         runs contract checks                                                              local command\n",
      "list          returns all available spiders in the current project                              local command\n",
      "edit          edits the given spider                                                            local command\n",
      "parse         fetches the given URL and parses it                                               local command\n",
      "bench         runs a quick benchmark test                                                       local command\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"remove-input\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "data = [[\"startproject\", \"creates a new Scrapy project\", \"global command\"], \n",
    "        [\"genspider\", \"creates a new spider on the current folder\", \"global command\"], \n",
    "        [\"settings\", \"gets the value of a Scrapy setting\", \"global command\"], \n",
    "        [\"runspider\", \"run a spider self-contained in a Python file, without having to create a project\", \"global command\"],\n",
    "        [\"shell\", \"starts the Scrapy shell for the given URL\", \"global command\"],\n",
    "        [\"fetch\", \"downloads the given URL and writes the contents to standard output\", \"global command\"],\n",
    "        [\"view\", \"Opens the given URL in a browser, as your Scrapy spider would see it\", \"global command\"],\n",
    "        [\"version\", \"prints the Scrapy version\", \"global command\"],\n",
    "        [\"crawl\", \"starts crawling with spider\", \"local command\"],\n",
    "        [\"check\", \"runs contract checks\", \"local command\"],\n",
    "        [\"list\", \"returns all available spiders in the current project\", \"local command\"],\n",
    "        [\"edit\", \"edits the given spider\", \"local command\"],\n",
    "        [\"parse\", \"fetches the given URL and parses it \", \"local command\"],\n",
    "        [\"bench\", \"runs a quick benchmark test\", \"local command\"]]\n",
    "  \n",
    "#define header names\n",
    "col_names = [\"Command\", \"Use\", \"Type\"]\n",
    "  \n",
    "#display table\n",
    "print(tabulate(data, headers = col_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our project and spider\n",
    "Now, we can start on our project and spider. We scrape data from [worldometers](https://www.worldometers.info/world-population/population-by-country/) to extract and save populations by country.\n",
    "\n",
    "First, open the terminal and run the line to crreate a project, <b>spider_worldometer</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4162788128.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [8]\u001b[0;36m\u001b[0m\n\u001b[0;31m    scrapy startproject spider-worldometer\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy startproject spider_worldometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look around the folder you just created! And remember to update your working directory. Run the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cd spider_worldometer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project folder, create a spider, <b>worldometer</b>, and assign a URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy genspider worldometer https://www.worldometers.info/world-population/population-by-country/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your spider <b>worldometer</b> will create a file, <i>worldometer.py</i>, and it will something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometer'\n",
    "    allowed_domains = ['www.worldoueters.info/world-population/population-by-country']\n",
    "    start_urls = ['https://www.worildometers.info/vorld-population/population-by=country/']\n",
    "\n",
    "def parse(self, response):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out www.worldometer.info\n",
    "In order to scrape the data from this website, we need to learn a little bit more about how it is storing information in HTML. In web design, if a webiste is a human body, HTML is our bones, CSS is our skin, and JavaScript is our movement. Here, we want to focus on the bones ‚Äì the text!\n",
    "\n",
    "First, visit [worldometers](https://www.worldometers.info/world-population/population-by-country/) and right click to <i>inspect</i>.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./assets/inspect.png\" alt=\"scrapy\" width=\"800\"/><p>\n",
    "\n",
    "On Google Chrome, click on the cursor icon on the upper left corner of the right panel to inspect by element. \n",
    "<p style=\"text-align:center;\"><img src=\"./assets/inspect-by-element.png\" alt=\"scrapy\" width=\"800\"/><p>\n",
    "<p style=\"text-align:center;\"><img src=\"./assets/inspect-td-a.png\" alt=\"scrapy\" width=\"800\"/><p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the shell for URL\n",
    "Let's explore how elements in the URL look like and how you can access them. On the terminal, run the lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy shell\n",
    ">>> r = scrapy.Request(url = 'https://www.worildometers.info/vorld-population/population-by=country/')\n",
    ">>> fetch(r)\n",
    ">>> response.body\n",
    ">>> response.xpath('//h1/text()').get() # returns titles\n",
    ">>> response.xpath('//td/a/text()').getall() # returns countries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the spider\n",
    "Update your spider with what you looked at when you started the shell. Your file, <i>worldometer.py</i>, will now look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometer'\n",
    "    allowed_domains = ['www.worldoueters.info/world-population/population-by-country']\n",
    "    start_urls = ['https://www.worildometers.info/vorld-population/population-by=country/']\n",
    "\n",
    "def parse(self, response):\n",
    "    # Extracting title and country names\n",
    "    title = response.xpath('//h1/text()').get()\n",
    "    countries = response.xpath('//td/a/text()').getall() \n",
    "    \n",
    "    # Return data extracted\n",
    "    yield {\n",
    "        'titles': title,\n",
    "        'countries': countries,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can start crawling with your spider, <b>worldometer</b>. On the terminal, run the line to get a list of all the countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy crawl worldometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data extracted as a .csv file\n",
    "Finally, let's update the spider to save the extracted data. Notice where the data for each row is contained. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./assets/inspect-by-element-row.png\" alt=\"scrapy\" width=\"800\"/><p>\n",
    "\n",
    "Your file, <i>worldometer.py</i>, will now look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class WorldometersSpider(scrapy.Spider):\n",
    "    name = 'worldometer'\n",
    "    allowed_domains = ['www.worldoueters.info/world-population/population-by-country']\n",
    "    start_urls = ['https://www.worildometers.info/vorld-population/population-by=country/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        rows = response.xpath('//tr')\n",
    "\n",
    "        for row in rows:\n",
    "            countries = row.xpath('./td/a/text()').get()\n",
    "            population = row.xpath('./td[3]/text()').get()\n",
    "        \n",
    "            yield {\n",
    "                'countries': countries,\n",
    "                'population': population,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, on the terminal, run the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scrapy crawl worldometer -o population.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚Ä¶and now we make it pretty!\n",
    "Learn more about data visualization libraries and visit these notebooks: \n",
    "<ul>\n",
    "<li> <a href = https://github.com/adelaiderobinson/Group_4_Plotly>Plotly</a>, an open source interactive graphing library</li>\n",
    "<li><a href = https://github.com/gabriellensmith/eds-217-scipy-tutorial>SciPy</a>, a library for fundamental algorithms for scientific computing</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('eds-217-scrapy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4fbe52cfab8287af63b0eb1ac789cb8cbde0e752af4f0b42da5714bf6f1e269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
